{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "namespace(aug='logarithmic', dropout=0.5, epochs=100, layers=[64, 64], lr=0.001, model='gcn')\n",
      "ModelID:  aug.logarithmic.dropout.0.5.epochs.100.layers.6464.lr.0.001.model.gcn.\n",
      "GBYOL(\n",
      "  (online_encoder): Encoder(\n",
      "    (stacked_gnn): ModuleList(\n",
      "      (0): GCNConv(93, 64)\n",
      "      (1): GCNConv(64, 64)\n",
      "    )\n",
      "    (encoder_norm): Normalise(\n",
      "      (norm): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (projection_head): Sequential(\n",
      "      (0): Linear(in_features=64, out_features=64, bias=True)\n",
      "      (1): Normalise(\n",
      "        (norm): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (2): ReLU(inplace=True)\n",
      "      (3): Dropout(p=0.5, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (online_predictor): Sequential(\n",
      "    (0): Linear(in_features=64, out_features=64, bias=True)\n",
      "    (1): Normalise(\n",
      "      (norm): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Dropout(p=0.5, inplace=False)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\apps\\311GIN_torch\\Lib\\site-packages\\torch_geometric\\deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n",
      "c:\\apps\\311GIN_torch\\Lib\\site-packages\\torch_geometric\\data\\in_memory_dataset.py:301: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46/100, batch 1, loss 2.90812.908095598220825 2.9080638885498047\n",
      "Epoch 54/100, batch 1, loss 2.88682.8868188858032227 2.8866446018218994\n",
      "Epoch 77/100, batch 1, loss 2.82652.8265271186828613 2.82535982131958\n",
      "Epoch 84/100, batch 1, loss 2.80572.80566668510437 2.8005683422088623\n",
      "Epoch 89/100, batch 1, loss 2.78922.7891924381256104 2.7882132530212402\n",
      "Epoch 92/100, batch 1, loss 2.78132.78128981590271 2.7744922637939453\n",
      "Epoch 95/100, batch 1, loss 2.76752.767542600631714 2.7665793895721436\n",
      "Epoch 100/100, batch 1, loss 2.7465\n",
      "data\\model/aug.logarithmic.dropout.0.5.epochs.100.layers.6464.lr.0.001.model.gcn..ep.*.pt\n",
      "The best epoch is: 10\n",
      "with training illicit f1: 0.2179355310897785 std: 0.013302375296915138\n",
      "with valid illicit f1: 0.38033947557101494 std: 0.020370164827516187\n",
      "namespace(aug='logarithmic', dropout=0.5, epochs=100, layers=[64, 64], lr=0.001, model='gat')\n",
      "ModelID:  aug.logarithmic.dropout.0.5.epochs.100.layers.6464.lr.0.001.model.gat.\n",
      "GBYOL(\n",
      "  (online_encoder): Encoder(\n",
      "    (stacked_gnn): ModuleList(\n",
      "      (0): GATConv(93, 8, heads=8)\n",
      "      (1): GATConv(64, 8, heads=8)\n",
      "    )\n",
      "    (encoder_norm): Normalise(\n",
      "      (norm): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (projection_head): Sequential(\n",
      "      (0): Linear(in_features=64, out_features=64, bias=True)\n",
      "      (1): Normalise(\n",
      "        (norm): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (2): ReLU(inplace=True)\n",
      "      (3): Dropout(p=0.5, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (online_predictor): Sequential(\n",
      "    (0): Linear(in_features=64, out_features=64, bias=True)\n",
      "    (1): Normalise(\n",
      "      (norm): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Dropout(p=0.5, inplace=False)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\apps\\311GIN_torch\\Lib\\site-packages\\torch_geometric\\deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n",
      "c:\\apps\\311GIN_torch\\Lib\\site-packages\\torch_geometric\\data\\in_memory_dataset.py:301: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36/100, batch 1, loss 2.94552.945547103881836 2.9420087337493896\n",
      "Epoch 38/100, batch 1, loss 2.94242.9423882961273193 2.9405078887939453\n",
      "Epoch 40/100, batch 1, loss 2.94362.943580389022827 2.9390523433685303\n",
      "Epoch 41/100, batch 1, loss 2.94222.942199230194092 2.9390523433685303\n",
      "Epoch 42/100, batch 1, loss 2.94092.94085693359375 2.9390523433685303\n",
      "Epoch 43/100, batch 1, loss 2.94062.9406473636627197 2.9390523433685303\n",
      "Epoch 44/100, batch 1, loss 2.94242.9423866271972656 2.9390523433685303\n",
      "Early stopping ***** epoch:  44\n",
      "\n",
      "data\\model/aug.logarithmic.dropout.0.5.epochs.100.layers.6464.lr.0.001.model.gat..ep.*.pt\n",
      "The best epoch is: 10\n",
      "with training illicit f1: 0.2556331603272271 std: 0.027206617049294876\n",
      "with valid illicit f1: 0.5923932201793818 std: 0.014785525449936512\n",
      "namespace(aug='logarithmic', dropout=0.5, epochs=100, layers=[64, 64], lr=0.001, model='sage')\n",
      "ModelID:  aug.logarithmic.dropout.0.5.epochs.100.layers.6464.lr.0.001.model.sage.\n",
      "GBYOL(\n",
      "  (online_encoder): Encoder(\n",
      "    (stacked_gnn): ModuleList(\n",
      "      (0): SAGEConv(93, 64, aggr=mean)\n",
      "      (1): SAGEConv(64, 64, aggr=mean)\n",
      "    )\n",
      "    (encoder_norm): Normalise(\n",
      "      (norm): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (projection_head): Sequential(\n",
      "      (0): Linear(in_features=64, out_features=64, bias=True)\n",
      "      (1): Normalise(\n",
      "        (norm): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (2): ReLU(inplace=True)\n",
      "      (3): Dropout(p=0.5, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (online_predictor): Sequential(\n",
      "    (0): Linear(in_features=64, out_features=64, bias=True)\n",
      "    (1): Normalise(\n",
      "      (norm): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Dropout(p=0.5, inplace=False)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\apps\\311GIN_torch\\Lib\\site-packages\\torch_geometric\\deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n",
      "c:\\apps\\311GIN_torch\\Lib\\site-packages\\torch_geometric\\data\\in_memory_dataset.py:301: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34/100, batch 1, loss 2.97622.9761745929718018 2.9756593704223633\n",
      "Epoch 36/100, batch 1, loss 2.97492.974940299987793 2.974130630493164\n",
      "Epoch 37/100, batch 1, loss 2.97812.9780983924865723 2.974130630493164\n",
      "Epoch 40/100, batch 1, loss 2.97592.975876569747925 2.9730398654937744\n",
      "Epoch 41/100, batch 1, loss 2.97432.974290370941162 2.9730398654937744\n",
      "Epoch 42/100, batch 1, loss 2.97492.97489595413208 2.9730398654937744\n",
      "Epoch 43/100, batch 1, loss 2.97552.975487232208252 2.9730398654937744\n",
      "Epoch 44/100, batch 1, loss 2.97382.973771095275879 2.9730398654937744\n",
      "Early stopping ***** epoch:  44\n",
      "\n",
      "data\\model/aug.logarithmic.dropout.0.5.epochs.100.layers.6464.lr.0.001.model.sage..ep.*.pt\n",
      "The best epoch is: 10\n",
      "with training illicit f1: 0.5324077746999013 std: 0.008041912553393782\n",
      "with valid illicit f1: 0.6167798710439469 std: 0.012713829916994504\n",
      "namespace(aug='logarithmic', dropout=0.5, epochs=100, layers=[128, 128], lr=0.001, model='gcn')\n",
      "ModelID:  aug.logarithmic.dropout.0.5.epochs.100.layers.128128.lr.0.001.model.gcn.\n",
      "GBYOL(\n",
      "  (online_encoder): Encoder(\n",
      "    (stacked_gnn): ModuleList(\n",
      "      (0): GCNConv(93, 128)\n",
      "      (1): GCNConv(128, 128)\n",
      "    )\n",
      "    (encoder_norm): Normalise(\n",
      "      (norm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (projection_head): Sequential(\n",
      "      (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (1): Normalise(\n",
      "        (norm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (2): ReLU(inplace=True)\n",
      "      (3): Dropout(p=0.5, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (online_predictor): Sequential(\n",
      "    (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (1): Normalise(\n",
      "      (norm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Dropout(p=0.5, inplace=False)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\apps\\311GIN_torch\\Lib\\site-packages\\torch_geometric\\deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n",
      "c:\\apps\\311GIN_torch\\Lib\\site-packages\\torch_geometric\\data\\in_memory_dataset.py:301: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/100, batch 1, loss 2.94372.943746328353882 2.942883253097534\n",
      "Epoch 29/100, batch 1, loss 2.94432.94431471824646 2.942883253097534\n",
      "Epoch 31/100, batch 1, loss 2.94442.9443888664245605 2.942385196685791\n",
      "Epoch 32/100, batch 1, loss 2.94272.9426538944244385 2.942385196685791\n",
      "Epoch 33/100, batch 1, loss 2.94482.9447872638702393 2.942385196685791\n",
      "Epoch 34/100, batch 1, loss 2.94562.9456026554107666 2.942385196685791\n",
      "Epoch 35/100, batch 1, loss 2.94462.944636344909668 2.942385196685791\n",
      "Early stopping ***** epoch:  35\n",
      "\n",
      "data\\model/aug.logarithmic.dropout.0.5.epochs.100.layers.128128.lr.0.001.model.gcn..ep.*.pt\n",
      "The best epoch is: 10\n",
      "with training illicit f1: 0.28135323379651656 std: 0.015977461468785866\n",
      "with valid illicit f1: 0.44678190260267614 std: 0.01975235217002004\n",
      "namespace(aug='logarithmic', dropout=0.5, epochs=100, layers=[128, 128], lr=0.001, model='gat')\n",
      "ModelID:  aug.logarithmic.dropout.0.5.epochs.100.layers.128128.lr.0.001.model.gat.\n",
      "GBYOL(\n",
      "  (online_encoder): Encoder(\n",
      "    (stacked_gnn): ModuleList(\n",
      "      (0): GATConv(93, 16, heads=8)\n",
      "      (1): GATConv(128, 16, heads=8)\n",
      "    )\n",
      "    (encoder_norm): Normalise(\n",
      "      (norm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (projection_head): Sequential(\n",
      "      (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (1): Normalise(\n",
      "        (norm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (2): ReLU(inplace=True)\n",
      "      (3): Dropout(p=0.5, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (online_predictor): Sequential(\n",
      "    (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (1): Normalise(\n",
      "      (norm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Dropout(p=0.5, inplace=False)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\apps\\311GIN_torch\\Lib\\site-packages\\torch_geometric\\deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n",
      "c:\\apps\\311GIN_torch\\Lib\\site-packages\\torch_geometric\\data\\in_memory_dataset.py:301: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/100, batch 1, loss 2.93582.9357619285583496 2.9355528354644775\n",
      "Epoch 28/100, batch 1, loss 2.92542.9253547191619873 2.925347089767456\n",
      "Epoch 29/100, batch 1, loss 2.92662.926557779312134 2.925347089767456\n",
      "Epoch 34/100, batch 1, loss 2.92222.922170400619507 2.9216809272766113\n",
      "Epoch 35/100, batch 1, loss 2.92232.9223408699035645 2.9216809272766113\n",
      "Epoch 37/100, batch 1, loss 2.91932.919264078140259 2.9187283515930176\n",
      "Epoch 38/100, batch 1, loss 2.92042.920365810394287 2.9187283515930176\n",
      "Epoch 39/100, batch 1, loss 2.92062.9205780029296875 2.9187283515930176\n",
      "Epoch 40/100, batch 1, loss 2.91922.9192347526550293 2.9187283515930176\n",
      "Epoch 41/100, batch 1, loss 2.91882.918773889541626 2.9187283515930176\n",
      "Early stopping ***** epoch:  41\n",
      "\n",
      "data\\model/aug.logarithmic.dropout.0.5.epochs.100.layers.128128.lr.0.001.model.gat..ep.*.pt\n",
      "The best epoch is: 10\n",
      "with training illicit f1: 0.29819893257399216 std: 0.03395547933325426\n",
      "with valid illicit f1: 0.6157585619372998 std: 0.012202962628505737\n",
      "namespace(aug='logarithmic', dropout=0.5, epochs=100, layers=[128, 128], lr=0.001, model='sage')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\apps\\311GIN_torch\\Lib\\site-packages\\torch_geometric\\deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n",
      "c:\\apps\\311GIN_torch\\Lib\\site-packages\\torch_geometric\\data\\in_memory_dataset.py:301: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModelID:  aug.logarithmic.dropout.0.5.epochs.100.layers.128128.lr.0.001.model.sage.\n",
      "GBYOL(\n",
      "  (online_encoder): Encoder(\n",
      "    (stacked_gnn): ModuleList(\n",
      "      (0): SAGEConv(93, 128, aggr=mean)\n",
      "      (1): SAGEConv(128, 128, aggr=mean)\n",
      "    )\n",
      "    (encoder_norm): Normalise(\n",
      "      (norm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (projection_head): Sequential(\n",
      "      (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (1): Normalise(\n",
      "        (norm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (2): ReLU(inplace=True)\n",
      "      (3): Dropout(p=0.5, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (online_predictor): Sequential(\n",
      "    (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (1): Normalise(\n",
      "      (norm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Dropout(p=0.5, inplace=False)\n",
      "  )\n",
      ")\n",
      "Epoch 24/100, batch 1, loss 2.98452.984531879425049 2.9842352867126465\n",
      "Epoch 27/100, batch 1, loss 2.98172.9816882610321045 2.9808475971221924\n",
      "Epoch 28/100, batch 1, loss 2.98092.9808509349823 2.9808475971221924\n",
      "Epoch 29/100, batch 1, loss 2.98212.9820735454559326 2.9808475971221924\n",
      "Epoch 30/100, batch 1, loss 2.98322.9832000732421875 2.9808475971221924\n",
      "Epoch 31/100, batch 1, loss 2.98372.9836885929107666 2.9808475971221924\n",
      "Early stopping ***** epoch:  31\n",
      "\n",
      "data\\model/aug.logarithmic.dropout.0.5.epochs.100.layers.128128.lr.0.001.model.sage..ep.*.pt\n",
      "The best epoch is: 10\n",
      "with training illicit f1: 0.5687629371345897 std: 0.010545025226364064\n",
      "with valid illicit f1: 0.6465783340305075 std: 0.006274666133129891\n",
      "namespace(aug='rotate', dropout=0.5, epochs=100, layers=[64, 64], lr=0.001, model='gcn')\n",
      "ModelID:  aug.rotate.dropout.0.5.epochs.100.layers.6464.lr.0.001.model.gcn.\n",
      "GBYOL(\n",
      "  (online_encoder): Encoder(\n",
      "    (stacked_gnn): ModuleList(\n",
      "      (0): GCNConv(93, 64)\n",
      "      (1): GCNConv(64, 64)\n",
      "    )\n",
      "    (encoder_norm): Normalise(\n",
      "      (norm): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (projection_head): Sequential(\n",
      "      (0): Linear(in_features=64, out_features=64, bias=True)\n",
      "      (1): Normalise(\n",
      "        (norm): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (2): ReLU(inplace=True)\n",
      "      (3): Dropout(p=0.5, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (online_predictor): Sequential(\n",
      "    (0): Linear(in_features=64, out_features=64, bias=True)\n",
      "    (1): Normalise(\n",
      "      (norm): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Dropout(p=0.5, inplace=False)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\apps\\311GIN_torch\\Lib\\site-packages\\torch_geometric\\deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n",
      "c:\\apps\\311GIN_torch\\Lib\\site-packages\\torch_geometric\\data\\in_memory_dataset.py:301: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38/100, batch 1, loss 2.95792.9579050540924072 2.957031726837158\n",
      "Epoch 39/100, batch 1, loss 2.95712.9571456909179688 2.957031726837158\n",
      "Epoch 44/100, batch 1, loss 2.95282.952812910079956 2.951418876647949\n",
      "Epoch 45/100, batch 1, loss 2.95172.9516751766204834 2.951418876647949\n",
      "Epoch 47/100, batch 1, loss 2.94952.9494969844818115 2.949402332305908\n",
      "Epoch 50/100, batch 1, loss 2.94892.9488587379455566 2.9458696842193604\n",
      "Epoch 52/100, batch 1, loss 2.94832.948263645172119 2.945770740509033\n",
      "Epoch 53/100, batch 1, loss 2.94862.948607921600342 2.945770740509033\n",
      "Epoch 55/100, batch 1, loss 2.94762.947619676589966 2.945314884185791\n",
      "Epoch 57/100, batch 1, loss 2.94592.945934534072876 2.944430112838745\n",
      "Epoch 58/100, batch 1, loss 2.94682.9468307495117188 2.944430112838745\n",
      "Epoch 59/100, batch 1, loss 2.95092.950899362564087 2.944430112838745\n",
      "Epoch 60/100, batch 1, loss 2.94812.9480557441711426 2.944430112838745\n",
      "Epoch 61/100, batch 1, loss 2.95052.9504942893981934 2.944430112838745\n",
      "Early stopping ***** epoch:  61\n",
      "\n",
      "data\\model/aug.rotate.dropout.0.5.epochs.100.layers.6464.lr.0.001.model.gcn..ep.*.pt\n",
      "The best epoch is: 10\n",
      "with training illicit f1: 0.1958333049592588 std: 0.01010046851618959\n",
      "with valid illicit f1: 0.36767060317252814 std: 0.02702207269195753\n",
      "namespace(aug='rotate', dropout=0.5, epochs=100, layers=[64, 64], lr=0.001, model='gat')\n",
      "ModelID:  aug.rotate.dropout.0.5.epochs.100.layers.6464.lr.0.001.model.gat.\n",
      "GBYOL(\n",
      "  (online_encoder): Encoder(\n",
      "    (stacked_gnn): ModuleList(\n",
      "      (0): GATConv(93, 8, heads=8)\n",
      "      (1): GATConv(64, 8, heads=8)\n",
      "    )\n",
      "    (encoder_norm): Normalise(\n",
      "      (norm): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (projection_head): Sequential(\n",
      "      (0): Linear(in_features=64, out_features=64, bias=True)\n",
      "      (1): Normalise(\n",
      "        (norm): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (2): ReLU(inplace=True)\n",
      "      (3): Dropout(p=0.5, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (online_predictor): Sequential(\n",
      "    (0): Linear(in_features=64, out_features=64, bias=True)\n",
      "    (1): Normalise(\n",
      "      (norm): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Dropout(p=0.5, inplace=False)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\apps\\311GIN_torch\\Lib\\site-packages\\torch_geometric\\deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n",
      "c:\\apps\\311GIN_torch\\Lib\\site-packages\\torch_geometric\\data\\in_memory_dataset.py:301: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/100, batch 1, loss 2.99842.99841046333313 2.9964945316314697\n",
      "Epoch 34/100, batch 1, loss 2.97682.976841449737549 2.9765100479125977\n",
      "Epoch 38/100, batch 1, loss 2.96852.9684720039367676 2.964498519897461\n",
      "Epoch 41/100, batch 1, loss 2.96532.9652748107910156 2.9587738513946533\n",
      "Epoch 42/100, batch 1, loss 2.96282.962824821472168 2.9587738513946533\n",
      "Epoch 43/100, batch 1, loss 2.96102.9610140323638916 2.9587738513946533\n",
      "Epoch 45/100, batch 1, loss 2.95832.958339214324951 2.955676317214966\n",
      "Epoch 46/100, batch 1, loss 2.95592.9558897018432617 2.955676317214966\n",
      "Epoch 49/100, batch 1, loss 2.95502.955019474029541 2.953350782394409\n",
      "Epoch 54/100, batch 1, loss 2.95922.959150552749634 2.949540615081787\n",
      "Epoch 55/100, batch 1, loss 2.95002.949972152709961 2.949540615081787\n",
      "Epoch 56/100, batch 1, loss 2.95642.956425905227661 2.949540615081787\n",
      "Epoch 57/100, batch 1, loss 2.95522.955214500427246 2.949540615081787\n",
      "Epoch 58/100, batch 1, loss 2.95402.9539613723754883 2.949540615081787\n",
      "Early stopping ***** epoch:  58\n",
      "\n",
      "data\\model/aug.rotate.dropout.0.5.epochs.100.layers.6464.lr.0.001.model.gat..ep.*.pt\n",
      "The best epoch is: 10\n",
      "with training illicit f1: 0.2652355533849225 std: 0.007899819978644269\n",
      "with valid illicit f1: 0.586226420770983 std: 0.012683269449963138\n",
      "namespace(aug='rotate', dropout=0.5, epochs=100, layers=[64, 64], lr=0.001, model='sage')\n",
      "ModelID:  aug.rotate.dropout.0.5.epochs.100.layers.6464.lr.0.001.model.sage.\n",
      "GBYOL(\n",
      "  (online_encoder): Encoder(\n",
      "    (stacked_gnn): ModuleList(\n",
      "      (0): SAGEConv(93, 64, aggr=mean)\n",
      "      (1): SAGEConv(64, 64, aggr=mean)\n",
      "    )\n",
      "    (encoder_norm): Normalise(\n",
      "      (norm): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (projection_head): Sequential(\n",
      "      (0): Linear(in_features=64, out_features=64, bias=True)\n",
      "      (1): Normalise(\n",
      "        (norm): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (2): ReLU(inplace=True)\n",
      "      (3): Dropout(p=0.5, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (online_predictor): Sequential(\n",
      "    (0): Linear(in_features=64, out_features=64, bias=True)\n",
      "    (1): Normalise(\n",
      "      (norm): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Dropout(p=0.5, inplace=False)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\apps\\311GIN_torch\\Lib\\site-packages\\torch_geometric\\deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n",
      "c:\\apps\\311GIN_torch\\Lib\\site-packages\\torch_geometric\\data\\in_memory_dataset.py:301: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/100, batch 1, loss 2.99222.9921743869781494 2.9902079105377197\n",
      "Epoch 28/100, batch 1, loss 2.98702.987025499343872 2.9841840267181396\n",
      "Epoch 29/100, batch 1, loss 2.98542.9854140281677246 2.9841840267181396\n",
      "Epoch 32/100, batch 1, loss 2.98152.981456995010376 2.97833514213562\n",
      "Epoch 33/100, batch 1, loss 2.98272.982703685760498 2.97833514213562\n",
      "Epoch 34/100, batch 1, loss 2.98232.982304096221924 2.97833514213562\n",
      "Epoch 37/100, batch 1, loss 2.97992.9799396991729736 2.977381944656372\n",
      "Epoch 38/100, batch 1, loss 2.97922.9792003631591797 2.977381944656372\n",
      "Epoch 40/100, batch 1, loss 2.97932.979278564453125 2.9753048419952393\n",
      "Epoch 42/100, batch 1, loss 2.97942.979418992996216 2.97076153755188\n",
      "Epoch 43/100, batch 1, loss 2.97692.9768760204315186 2.97076153755188\n",
      "Epoch 44/100, batch 1, loss 2.97652.976515293121338 2.97076153755188\n",
      "Epoch 45/100, batch 1, loss 2.97542.9754230976104736 2.97076153755188\n",
      "Epoch 46/100, batch 1, loss 2.97352.9735424518585205 2.97076153755188\n",
      "Early stopping ***** epoch:  46\n",
      "\n",
      "data\\model/aug.rotate.dropout.0.5.epochs.100.layers.6464.lr.0.001.model.sage..ep.*.pt\n",
      "The best epoch is: 10\n",
      "with training illicit f1: 0.48299316418333227 std: 0.005957662909754509\n",
      "with valid illicit f1: 0.5978273519755856 std: 0.00666537887753658\n",
      "namespace(aug='rotate', dropout=0.5, epochs=100, layers=[128, 128], lr=0.001, model='gcn')\n",
      "ModelID:  aug.rotate.dropout.0.5.epochs.100.layers.128128.lr.0.001.model.gcn.\n",
      "GBYOL(\n",
      "  (online_encoder): Encoder(\n",
      "    (stacked_gnn): ModuleList(\n",
      "      (0): GCNConv(93, 128)\n",
      "      (1): GCNConv(128, 128)\n",
      "    )\n",
      "    (encoder_norm): Normalise(\n",
      "      (norm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (projection_head): Sequential(\n",
      "      (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (1): Normalise(\n",
      "        (norm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (2): ReLU(inplace=True)\n",
      "      (3): Dropout(p=0.5, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (online_predictor): Sequential(\n",
      "    (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (1): Normalise(\n",
      "      (norm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Dropout(p=0.5, inplace=False)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\apps\\311GIN_torch\\Lib\\site-packages\\torch_geometric\\deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n",
      "c:\\apps\\311GIN_torch\\Lib\\site-packages\\torch_geometric\\data\\in_memory_dataset.py:301: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/100, batch 1, loss 2.96342.9633567333221436 2.9626786708831787\n",
      "Epoch 33/100, batch 1, loss 2.95992.9599099159240723 2.958735227584839\n",
      "Epoch 37/100, batch 1, loss 2.95652.9564714431762695 2.956376791000366\n",
      "Epoch 38/100, batch 1, loss 2.95642.9564156532287598 2.956376791000366\n",
      "Epoch 40/100, batch 1, loss 2.95622.9561805725097656 2.953754425048828\n",
      "Epoch 44/100, batch 1, loss 2.95232.952338457107544 2.9522600173950195\n",
      "Epoch 47/100, batch 1, loss 2.94682.9468414783477783 2.9467761516571045\n",
      "Epoch 88/100, batch 1, loss 2.73992.739858627319336 2.7378673553466797\n",
      "Epoch 100/100, batch 1, loss 2.6717\n",
      "data\\model/aug.rotate.dropout.0.5.epochs.100.layers.128128.lr.0.001.model.gcn..ep.*.pt\n",
      "The best epoch is: 10\n",
      "with training illicit f1: 0.24925359275347528 std: 0.012549660555809554\n",
      "with valid illicit f1: 0.39420522225685956 std: 0.02069933030082848\n",
      "namespace(aug='rotate', dropout=0.5, epochs=100, layers=[128, 128], lr=0.001, model='gat')\n",
      "ModelID:  aug.rotate.dropout.0.5.epochs.100.layers.128128.lr.0.001.model.gat.\n",
      "GBYOL(\n",
      "  (online_encoder): Encoder(\n",
      "    (stacked_gnn): ModuleList(\n",
      "      (0): GATConv(93, 16, heads=8)\n",
      "      (1): GATConv(128, 16, heads=8)\n",
      "    )\n",
      "    (encoder_norm): Normalise(\n",
      "      (norm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (projection_head): Sequential(\n",
      "      (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (1): Normalise(\n",
      "        (norm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (2): ReLU(inplace=True)\n",
      "      (3): Dropout(p=0.5, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (online_predictor): Sequential(\n",
      "    (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (1): Normalise(\n",
      "      (norm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Dropout(p=0.5, inplace=False)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\apps\\311GIN_torch\\Lib\\site-packages\\torch_geometric\\deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n",
      "c:\\apps\\311GIN_torch\\Lib\\site-packages\\torch_geometric\\data\\in_memory_dataset.py:301: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/100, batch 1, loss 2.93962.939633846282959 2.9394311904907227\n",
      "Epoch 30/100, batch 1, loss 2.93872.938650608062744 2.9377663135528564\n",
      "Epoch 33/100, batch 1, loss 2.93922.9392247200012207 2.9353673458099365\n",
      "Epoch 35/100, batch 1, loss 2.93732.937281847000122 2.9351656436920166\n",
      "Epoch 37/100, batch 1, loss 2.93222.9321842193603516 2.93017840385437\n",
      "Epoch 38/100, batch 1, loss 2.93152.93151593208313 2.93017840385437\n",
      "Epoch 40/100, batch 1, loss 2.92822.928190231323242 2.9267170429229736\n",
      "Epoch 41/100, batch 1, loss 2.93092.930913209915161 2.9267170429229736\n",
      "Epoch 42/100, batch 1, loss 2.92992.929898500442505 2.9267170429229736\n",
      "Epoch 43/100, batch 1, loss 2.92712.9270901679992676 2.9267170429229736\n",
      "Epoch 44/100, batch 1, loss 2.92772.9276862144470215 2.9267170429229736\n",
      "Early stopping ***** epoch:  44\n",
      "\n",
      "data\\model/aug.rotate.dropout.0.5.epochs.100.layers.128128.lr.0.001.model.gat..ep.*.pt\n",
      "The best epoch is: 10\n",
      "with training illicit f1: 0.30189576187966033 std: 0.018063266453431797\n",
      "with valid illicit f1: 0.5798385027536723 std: 0.0463039415238996\n",
      "namespace(aug='rotate', dropout=0.5, epochs=100, layers=[128, 128], lr=0.001, model='sage')\n",
      "ModelID:  aug.rotate.dropout.0.5.epochs.100.layers.128128.lr.0.001.model.sage.\n",
      "GBYOL(\n",
      "  (online_encoder): Encoder(\n",
      "    (stacked_gnn): ModuleList(\n",
      "      (0): SAGEConv(93, 128, aggr=mean)\n",
      "      (1): SAGEConv(128, 128, aggr=mean)\n",
      "    )\n",
      "    (encoder_norm): Normalise(\n",
      "      (norm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (projection_head): Sequential(\n",
      "      (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (1): Normalise(\n",
      "        (norm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (2): ReLU(inplace=True)\n",
      "      (3): Dropout(p=0.5, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (online_predictor): Sequential(\n",
      "    (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (1): Normalise(\n",
      "      (norm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Dropout(p=0.5, inplace=False)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\apps\\311GIN_torch\\Lib\\site-packages\\torch_geometric\\deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n",
      "c:\\apps\\311GIN_torch\\Lib\\site-packages\\torch_geometric\\data\\in_memory_dataset.py:301: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/100, batch 1, loss 2.98112.9810662269592285 2.9807016849517822\n",
      "Epoch 24/100, batch 1, loss 2.98412.984123945236206 2.9787707328796387\n",
      "Epoch 25/100, batch 1, loss 2.98172.9816973209381104 2.9787707328796387\n",
      "Epoch 26/100, batch 1, loss 2.98272.982685089111328 2.9787707328796387\n",
      "Epoch 27/100, batch 1, loss 2.98432.9843027591705322 2.9787707328796387\n",
      "Epoch 28/100, batch 1, loss 2.98322.9831511974334717 2.9787707328796387\n",
      "Early stopping ***** epoch:  28\n",
      "\n",
      "data\\model/aug.rotate.dropout.0.5.epochs.100.layers.128128.lr.0.001.model.sage..ep.*.pt\n",
      "The best epoch is: 10\n",
      "with training illicit f1: 0.5379030226362904 std: 0.01400692571860864\n",
      "with valid illicit f1: 0.6346540636463606 std: 0.006166022419073106\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "The utils model contains the hyperparameters to be searched\n",
    "This look methodically works through the permutations,\n",
    "trains and evaluates the models.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from types import SimpleNamespace\n",
    "import utils\n",
    "import trainer\n",
    "\n",
    "\n",
    "all_params=utils.generate_args()\n",
    "for p in all_params:\n",
    "    args = SimpleNamespace(**p)\n",
    "    print (args)\n",
    "    trainer.train_validate(args)\n",
    "    \n",
    "     \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "311GIN_torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
